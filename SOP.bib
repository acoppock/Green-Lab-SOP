@article{Angrist1998,
author = {Angrist, Joshua D.},
journal = {Econometrica},
number = {2},
pages = {249--288},
title = {{Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on
Military Applicants}},
url = {http://www.jstor.org/stable/2998558},
volume = {66},
year = {1998}
}
@book{Angrist2009,
title={Mostly harmless econometrics: An empiricist's companion},
author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
publisher={Princeton University Press},
year = {2009}
}
@unpublished{Aronow2015,
author = {Aronow, Peter M. and Coppock, Alexander and Gerber, Alan S. and Green, Donald P. and Kern, Holger},
title = {{Double Sampling for Missing Outcome Data in Randomized Experiments}},
year = {2015}
}
@article{Bruhn2009,
author = {Bruhn, Miriam and McKenzie, David},
file = {:Users/alex/Documents/Dropbox/Columbia/Collaboration/work with winston/Green Lab SOP/Literature/Bruhn and McKenzie (2009).pdf:pdf},
journal = {American Economic Journal: Applied Economics},
number = {4},
pages = {200--232},
title = {{In Pursuit of Balance: Randomization in Practice in Development Field Experiments}},
url = {http://www.jstor.org/stable/25760187},
volume = {1},
year = {2009}
}
@article{Chung2013,
abstract = {Given independent samples from P and Q, two-sample permutation tests allow one to construct exact level tests when the null hypothesis is P=Q. On the other hand, when comparing or testing particular parameters \$\backslash theta\$ of P and Q, such as their means or medians, permutation tests need not be level \$\backslash alpha\$, or even approximately level \$\backslash alpha\$ in large samples. Under very weak assumptions for comparing estimators, we provide a general test procedure whereby the asymptotic validity of the permutation test holds while retaining the exact rejection probability \$\backslash alpha\$ in finite samples when the underlying distributions are identical. The ideas are broadly applicable and special attention is given to the k-sample problem of comparing general parameters, whereby a permutation test is constructed which is exact level \$\backslash alpha\$ under the hypothesis of identical distributions, but has asymptotic rejection probability \$\backslash alpha\$ under the more general null hypothesis of equality of parameters. A Monte Carlo simulation study is performed as well. A quite general theory is possible based on a coupling construction, as well as a key contiguity argument for the multinomial and multivariate hypergeometric distributions.},
archivePrefix = {arXiv},
arxivId = {1304.5939},
author = {Chung, EunYi and Romano, Joseph P.},
doi = {10.1214/13-AOS1090},
eprint = {1304.5939},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Behrens-Fisher problem,Coupling,Permutation test},
number = {2},
pages = {484--507},
title = {{Exact and Asymptotically Robust Permutation Tests}},
url = {http://projecteuclid.org/euclid.aos/1366138199},
volume = {41},
year = {2013}
}
@article{Firth1998,
abstract = {In the estimation of a population mean or total from a random sample, certain methods based on linear models are known to be automatically design consistent, regardless of how well the underlying model describes the population. A sufficient condition is identified for this type of robustness to model failure; the condition, which we call 'internal bias calibration', relates to the combination of a model and the method used to fit it. Included among the internally bias-calibrated models, in addition to the aforementioned linear models, are certain canonical link generalized linear models and nonparametric regressions constructed from them by a particular style of local likelihood fitting. Other models can often be made robust by using a suboptimal fitting method. Thus the class of model-based, but design consistent, analyses is enlarged to include more realistic models for certain types of survey variable such as binary indicators and counts. Particular applications discussed are the estimation of the size of a population subdomain, as arises in tax auditing for example, and the estimation of a bootstrap tail probability. © 1998 Royal Statistical Society.},
author = {Firth, D. and Bennett, K. E.},
doi = {10.1111/1467-9868.00105},
file = {:Users/alex/Documents/Dropbox/Columbia/Collaboration/work with winston/Green Lab SOP/Literature/Firth and Bennet (1998).pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B},
keywords = {auditing,bias calibration,bootstrap acceleration,cation,control variate,finite population,generalized linear model,importance sampling,instrumental variable,local likelihood,logistic,regression,smoothing,spline,strati,survey sampling},
number = {1},
pages = {3--21},
title = {{Robust Models in Probability Sampling}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00105/abstract},
volume = {60},
year = {1998}
}
@article{Freedman2008,
abstract = {The logit model is often used to analyze experimental data. However, randomization does not justify the model, so the usual estimators can be inconsistent. A consistent estimator is proposed. Neyman's non-parametric setup is used as a benchmark. In this setup, each subject has two potential responses, one if treated and the other if untreated; only one of the two responses can be observed. Beside the mathematics, there are simulation results, a brief review of the literature, and some recommendations for practice.},
archivePrefix = {arXiv},
arxivId = {0808.3914},
author = {Freedman, David A.},
doi = {10.1214/08-STS262},
eprint = {0808.3914},
file = {:Users/alex/Documents/Dropbox/Columbia/Collaboration/work with winston/Green Lab SOP/Literature/Freedman (2008).pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,average predicted probability},
number = {2},
pages = {237--249},
title = {{Randomization Does Not Justify Logistic Regression}},
url = {http://www.jstor.org/stable/27645896},
volume = {23},
year = {2008}
}
@book{Gerber2012,
address = {New York},
author = {Gerber, Alan S. and Green, Donald P.},
keywords = {Theory,full,ref},
mendeley-tags = {Theory,full,ref},
publisher = {W.W. Norton},
title = {{Field Experiments: Design, Analysis, and Interpretation}},
year = {2012}
}
@article{Gerber2014,
  title={Reporting guidelines for experimental research: A report from the experimental research section standards committee},
  author={Gerber, Alan and Arceneaux, Kevin and Boudreau, Cheryl and Dowling, Conor and Hillygus, Sunshine and Palfrey, Thomas and Biggers, Daniel R. and Hendry, David J.},
  doi={10.1017/xps.2014.11},
  journal={Journal of Experimental Political Science},
  volume={1},
  number={01},
  pages={81--98},
  year={2014}
}
@article{Greenberg2014,
author = {Greenberg, David and Barnow, Burt S.},
doi = {10.1177/0193841X14545782},
file = {:Users/alex/Documents/Dropbox/Columbia/Collaboration/work with winston/Green Lab SOP/Literature/Greenberg and Barnow (2014).pdf:pdf},
issn = {0193-841X},
journal = {Evaluation Review},
number = {5},
pages = {359--387},
title = {{Flaws in Evaluations of Social Programs: Illustrations From Randomized Controlled Trials}},
url = {http://erx.sagepub.com/cgi/doi/10.1177/0193841X14545782},
volume = {38},
year = {2014}
}
@article{Humphreys2013,
abstract = {Social scientists generally enjoy substantial latitude in selecting measures and models for hypothesis testing. Coupled with publication and related biases, this latitude raises the concern that researchers may intentionally or unintentionally select models that yield positive findings, leading to an unreliable body of published research. To combat this “fishing” problem in medical studies, leading journals now require pre- registration of designs that emphasize the prior identification of dependent and independent variables. However, we demonstrate here that even with this level of advanced specification, the scope for fishing is considerable when there is latitude over selection of covariates, subgroups, and other elements of an analysis plan. These concerns could be addressed through the use of a form of comprehensive registration. We experiment with such an approach in the context of an ongoing field experiment for which we drafted a complete “mock report” of findings using fake data on treatment assignment. We describe the advantages and disadvantages of this form of registration and propose that a comprehensive but nonbinding approach be adopted as a first step to combat fishing by social scientists. Likely effects of comprehensive but nonbinding registration are discussed, the principal advantage being communication rather than commit- ment, in particular that it generates a clear distinction between exploratory analyses and genuine tests.},
author = {Humphreys, Macartan and {Sanchez de la Sierra}, Raul and {van der Windt}, Peter},
doi = {10.1093/pan/mps021},
file = {:Users/alex/Documents/Dropbox/Columbia/Collaboration/work with winston/Green Lab SOP/Literature/Humphreys, de la Sierra, van der Windt (2013).pdf:pdf},
issn = {10471987},
journal = {Political Analysis},
number = {1},
pages = {1--20},
title = {{Fishing, Commitment, and Communication: A Proposal for Comprehensive Nonbinding Research Registration}},
url = {http://pan.oxfordjournals.org/content/21/1/1},
volume = {21},
year = {2013}
}
@unpublished{Judkins2014,
author = {Judkins, David R. and Porter, Kristin E.},
title = {{Robustness of Ordinary Least Squares in Randomized Clinical Trials}},
year = {2014}
}
@book{Krueger2004,
abstract = {This article reexamines data from the New York City school choice program, the largest and best-implemented private school scholarship experiment yet conducted. In the experiment, low-income public school students in kindergarten to Grade 4 were eligible to participate in a series of lotteries for a private school scholarship in May 1997. Data were collected from students and their parents at baseline and in the spring of each of the next 3 years. Students with missing baseline test scores, which encompasses all those who were initially in kindergarten and 11\% of those initially in Grades 1 to 4, were excluded from previous analyses of achievement, even though these students were tested in the follow-up years. In principle, random assignment would be expected to lead treatment status to be uncorrelated with all baseline characteristics. In addition, it was found that the effect of vouchers was sensitive to the particular way race/ethnicity was defined. (PsycINFO Database Record (c) 2010 APA, all rights reserved) (journal abstract)},
author = {Krueger, Alan B. and Zhu, Pei},
booktitle = {American Behavioral Scientist},
doi = {10.1177/0002764203260152},
file = {:Users/alex/Documents/Dropbox/Columbia/Collaboration/work with winston/Green Lab SOP/Literature/Krueger and Zhu (2004).pdf:pdf},
isbn = {0002764203260},
issn = {00027642},
keywords = {and parents will seriously,children in private religious,consider the desirability,court has ruled in,many states,may be used to,now that the supreme,public funds,randomized experiment,school districts,school vouchers,schools,support vouchers to enroll,the zelman case that},
number = {5},
pages = {658--698},
title = {{Another Look at the New York City School Voucher Experiment}},
url = {http://abs.sagepub.com/content/47/5/658.short},
volume = {47},
year = {2004}
}
@article{Lee2009,
abstract = {This paper empirically assesses the wage effects of the Job Corps program, one of the largest federally funded job training programs in the U.S. Even with the aid of a randomized experiment, the impact of a training program on wages is difficult to study because of sample selection, a pervasive prob- lem in applied microeconometric research. Wage rates are only observed for those who are employed, and employment status itself may be affected by the training program. This paper develops an intuitive trimming procedure for bounding average treatment effects in the presence of sample selection. In con- trast to existing methods, the procedure requires neither exclusion restrictions nor a bounded support for the outcome of interest. Identification results, estimators, and their asymptotic distribution are pre- sented. The bounds suggest that the program raised wages, consistent with the notion that the Job Corps raises earnings by increasing human capital, rather than solely through encouraging work. The estima- tor is generally applicable to typical treatment evaluation problems in which there is nonrandom sample selection/attrition.},
author = {Lee, David S.},
doi = {10.1111/j.1467-937X.2009.00536.x},
file = {:Users/alex/Documents/Dropbox/Columbia/Collaboration/work with winston/Green Lab SOP/Literature/Lee (2009).pdf:pdf},
issn = {00346527},
journal = {Review of Economic Studies},
number = {3},
pages = {1071--1102},
title = {{Training, Wages, and Sample Selection: Estimating Sharp Bounds on Treatment Effects}},
volume = {76},
year = {2009}
}
@article{Lin2013,
author = {Lin, Winston},
doi = {10.1214/12-AOAS583},
file = {:Users/alex/Library/Application Support/Mendeley Desktop/Downloaded/Lin - 2013 - Agnostic Notes on Regression Adjustments to Experimental Data Reexamining Freedman’s Critique.pdf:pdf},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
keywords = {analysis of covariance,and phrases,covariate adjustment,program evaluation,randomization inference,robust standard errors,sandwich estimator,social experiments},
month = mar,
number = {1},
pages = {295--318},
title = {{Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique}},
url = {http://projecteuclid.org/euclid.aoas/1365527200},
volume = {7},
year = {2013}
}
@article{Lin2012a,
author = {Lin, Winston},
title = {{Regression Adjustment in Randomized Experiments: Is the Cure Really Worse than the Disease? Part I}},
url = {http://web.archive.org/web/20150505184132/http://blogs.worldbank.org/impactevaluations/node/847},
year = {2012}
}
@article{Lin2012b,
author = {Lin, Winston},
title = {{Regression Adjustment in Randomized Experiments: Is the Cure Really Worse than the Disease? Part II}},
url = {http://web.archive.org/web/20150505184245/http://blogs.worldbank.org/impactevaluations/node/849},
year = {2012}
}
@unpublished{LinGreen2015,
author = {Lin, Winston and Green, Donald P.},
title = {{Standard Operating Procedures: A Safety Net for Pre-Analysis
Plans}},
year = {2015}
}
@incollection{Mackinnon2013,
  title={{Thirty Years of Heteroskedasticity-Robust Inference}},
  author={MacKinnon, James G.},
  editor={Chen, Xiaohong and Swanson, Norman R.},
  booktitle={{Recent Advances and Future Directions in Causality, Prediction, and Specification Analysis: Essays in Honor of Halbert L. White Jr}},
  pages={437--461},
  year={2013},
  publisher={Springer}
}
@article{Morgan2012,
  title={Rerandomization to Improve Covariate Balance in Experiments},
  author={Morgan, Kari Lock and Rubin, Donald B.},
  doi={10.1214/12-AOS1008},
  journal={The Annals of Statistics},
  volume={40},
  number={2},
  pages={1263--1282},
  year={2012}
}
@article{Nickerson2005,
abstract = {Experiments conducted in the field allay concerns over external validity but are subject to the pitfalls of fieldwork. This article proves that scalable protocols conserve statistical efficiency in the face of problems implementing the treatment regime. Three designs are considered: randomly ordering the application of the treatment; matching subjects into groups prior to assignment; and placebo-controlled experiments. Three examples taken from voter mobilization field experiments demonstrate the utility of the design principles discussed.},
author = {Nickerson, David W.},
doi = {10.1093/pan/mpi015},
file = {:Users/alex/Library/Application Support/Mendeley Desktop/Downloaded/Nickerson - 2005 - Scalable Protocols Offer Efficient Design for Field Experiments.pdf:pdf},
isbn = {1047-1987},
issn = {10471987},
journal = {Political Analysis},
month = may,
number = {3},
pages = {233--252},
title = {{Scalable Protocols Offer Efficient Design for Field Experiments}},
url = {http://pan.oxfordjournals.org/content/13/3/233.abstract},
volume = {13},
year = {2005}
}
@article{Olken2015,
author = {Olken, Benjamin A.},
doi = {10.1257/jep.29.3.61},
journal = {Journal of Economic Perspectives},
title = {{Promises and Perils of Pre-Analysis Plans}},
volume = {29},
number = {3},
pages = {61--80},
year = {2015}
}
@article{Permutt1990,
abstract = {Results of a controlled experiment are often adjusted for covariates found by a preliminary test to differ significantly between the treatment and control groups. The resulting test's true significance level is lower than the nominal level. Greater power can be achieved by always adjusting for a covariate that is highly correlated with the response regardless of its distribution between groups.},
author = {Permutt, Thomas},
doi = {10.1002/sim.4780091209},
file = {:Users/alex/Documents/Dropbox/Columbia/Collaboration/work with winston/Green Lab SOP/Literature/Permutt (1990).pdf:pdf},
isbn = {0277-6715},
issn = {0277-6715},
journal = {Statistics in medicine},
number = {12},
pages = {1455--1462},
pmid = {2281233},
title = {{Testing for Imbalance of Covariates in Controlled Experiments}},
url = {http://europepmc.org/abstract/med/2281233},
volume = {9},
year = {1990}
}
@misc{Romano2009,
abstract = {This article compares parametric and nonparametric approaches to statistical inference. It considers their advantages and disadvantages, and their areas of applicability. Although there is no clear comprehensive conclusion, the article finds that in simple problems in which Wilcoxon type tests and estimators apply, they may be recommended as the methods of choice.},
author = {Romano, Joseph P.},
booktitle = {Journal of Nonparametric Statistics},
doi = {10.1080/10485250902846900},
isbn = {1048525090},
issn = {1048-5252},
number = {4},
pages = {419--424},
title = {{Discussion of ‘Parametric versus Nonparametrics: Two Alternative Methodologies’}},
url = {Discussion of ‘Parametric versus nonparametrics: two alternative methodologies’},
volume = {21},
year = {2009}
}
@book{Wooldridge2010,
abstract = {The second edition of this acclaimed graduate text provides a unified treatment of two methods used in contemporary econometric research, cross section and data panel methods. By focusing on assumptions that can be given behavioral content, the book maintains an appropriate level of rigor while emphasizing intuitive thinking. The analysis covers both linear and nonlinear models, including models with dynamics and/or individual heterogeneity. In addition to general estimation frameworks (particular methods of moments and maximum likelihood), specific linear and nonlinear methods are covered in detail, including probit and logit models and their multivariate, Tobit models, models for count data, censored and missing data schemes, causal (or treatment) effects, and duration analysis.Econometric Analysis of Cross Section and Panel Data was the first graduate econometrics text to focus on microeconomic data structures, allowing assumptions to be separated into population and sampling assumptions. This second edition has been substantially updated and revised. Improvements include a broader class of models for missing data problems; more detailed treatment of cluster problems, an important topic for empirical researchers; expanded discussion of "generalized instrumental variables" (GIV) estimation; new coverage (based on the author's own recent research) of inverse probability weighting; a more complete framework for estimating treatment effects with panel data, and a firmly established link between econometric approaches to nonlinear panel data and the "generalized estimating equation" literature popular in statistics and other fields. New attention is given to explaining when particular econometric methods can be applied; the goal is not only to tell readers what does work, but why certain "obvious" procedures do not. The numerous included exercises, both theoretical and computer-based, allow the reader to extend methods covered in the text and discover new insights.},
author = {Wooldridge, Jeffrey M},
edition = 2,
isbn = {0262232588},
pages = {1064},
publisher = {MIT Press},
title = {{Econometric Analysis of Cross Section and Panel Data}},
year = {2010}
}
